{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# MNIST VIB Example\n",
        "\n",
        "Here I demonstrate the Variational Information Bottleneck method on the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import math\n",
        "\n",
        "layers \u003d tf.contrib.layers\n",
        "ds \u003d tf.contrib.distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "# Turn on xla optimization\n",
        "config \u003d tf.ConfigProto()\n",
        "config.graph_options.optimizer_options.global_jit_level \u003d tf.OptimizerOptions.ON_1\n",
        "sess \u003d tf.InteractiveSession(config\u003dconfig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "scrolled": true,
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /tmp/mnistdata\\train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/mnistdata\\train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/mnistdata\\t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/mnistdata\\t10k-labels-idx1-ubyte.gz\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist_data \u003d input_data.read_data_sets(\u0027/tmp/mnistdata\u0027, validation_size\u003d0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\u0027mnist \u003d tf.keras.datasets.mnist\\n(x_train, y_train),(x_test, y_test) \u003d mnist.load_data()\\nmnist\u0027"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"mnist \u003d tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) \u003d mnist.load_data()\n",
        "mnist\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# data\n",
        "images \u003d tf.placeholder(tf.float32, [None, 784], \u0027images\u0027)\n",
        "labels \u003d tf.placeholder(tf.int64, [None], \u0027labels\u0027)\n",
        "one_hot_labels \u003d tf.one_hot(labels, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "# model\n\ndef encoder(images):\n    net \u003d layers.relu(2*images-1, 1024)\n    net \u003d layers.relu(net, 1024)\n    params \u003d layers.linear(net, 512)\n    mu, rho \u003d params[:, :256], params[:, 256:]\n    encoding \u003d ds.NormalWithSoftplusScale(mu, rho - 5.0)\n    return encoding\n\nwith tf.variable_scope(\u0027encoder\u0027):  # not important for the maths\n    encoding \u003d encoder(images)\n    \ndef decoder(encoding_sample):\n    net \u003d layers.linear(encoding_sample, 10)\n    return net\n\nwith tf.variable_scope(\u0027decoder\u0027):\n    logits \u003d decoder(encoding.sample())  \n    # logits are real numbers that will then be put into a softmax function to produce probabilities"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# the second term in the loss \n",
        "\n",
        "prior \u003d ds.Normal(0.0, 1.0)    \n",
        "    \n",
        "info_loss \u003d tf.reduce_sum(tf.reduce_mean(ds.kl_divergence(encoding, prior), axis\u003d0)) / math.log(2)\n",
        "\n",
        "IZX_bound \u003d info_loss  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# the first term in the loss \n",
        "\n",
        "class_loss \u003d tf.losses.softmax_cross_entropy(\n",
        "    logits\u003dlogits, onehot_labels\u003done_hot_labels) / math.log(2)\n",
        "\n",
        "IZY_bound \u003d math.log(10, 2) - class_loss  # first term is H(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# calculating loss J\n",
        "\n",
        "BETA \u003d 1e-3    \n",
        "\n",
        "total_loss \u003d class_loss + BETA * info_loss  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "accuracy \u003d tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), labels), tf.float32))\n",
        "\n",
        "with tf.variable_scope(\u0027decoder\u0027, reuse\u003dTrue):\n",
        "    many_logits \u003d decoder(encoding.sample(12))\n",
        "\n",
        "avg_accuracy \u003d tf.reduce_mean(tf.cast(tf.equal(tf.argmax(tf.reduce_mean(tf.nn.softmax(many_logits), 0), 1), labels), tf.float32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "batch_size \u003d 100\n",
        "steps_per_batch \u003d int(mnist_data.train.num_examples / batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "global_step \u003d tf.contrib.framework.get_or_create_global_step()\n",
        "learning_rate \u003d tf.train.exponential_decay(1e-4, global_step,\n",
        "                                           decay_steps\u003d2*steps_per_batch,\n",
        "                                           decay_rate\u003d0.97, staircase\u003dTrue)\n",
        "opt \u003d tf.train.AdamOptimizer(learning_rate, 0.5)\n",
        "\n",
        "ma \u003d tf.train.ExponentialMovingAverage(0.999, zero_debias\u003dTrue)\n",
        "ma_update \u003d ma.apply(tf.model_variables())\n",
        "\n",
        "saver \u003d tf.train.Saver()\n",
        "saver_polyak \u003d tf.train.Saver(ma.variables_to_restore())\n",
        "\n",
        "train_tensor \u003d tf.contrib.training.create_train_op(total_loss, opt,\n",
        "                                                   global_step,\n",
        "                                                   update_ops\u003d[ma_update])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "tf.global_variables_initializer().run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def evaluate():\n",
        "    IZY, IZX, acc, avg_acc \u003d sess.run([IZY_bound, IZX_bound, accuracy, avg_accuracy],\n",
        "                             feed_dict\u003d{images: mnist_data.test.images, labels: mnist_data.test.labels})\n",
        "    return IZY, IZX, acc, avg_acc, 1-acc, 1-avg_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0: IZY\u003d3.03\tIZX\u003d124.24\tacc\u003d0.9407\tavg_acc\u003d0.9497\terr\u003d0.0593\tavg_err\u003d0.0503\n",
            "1: IZY\u003d3.13\tIZX\u003d102.63\tacc\u003d0.9563\tavg_acc\u003d0.9660\terr\u003d0.0437\tavg_err\u003d0.0340\n",
            "2: IZY\u003d3.14\tIZX\u003d90.19\tacc\u003d0.9610\tavg_acc\u003d0.9708\terr\u003d0.0390\tavg_err\u003d0.0292\n",
            "3: IZY\u003d3.16\tIZX\u003d82.35\tacc\u003d0.9647\tavg_acc\u003d0.9732\terr\u003d0.0353\tavg_err\u003d0.0268\n",
            "4: IZY\u003d3.17\tIZX\u003d78.39\tacc\u003d0.9670\tavg_acc\u003d0.9740\terr\u003d0.0330\tavg_err\u003d0.0260\n",
            "5: IZY\u003d3.19\tIZX\u003d71.32\tacc\u003d0.9691\tavg_acc\u003d0.9768\terr\u003d0.0309\tavg_err\u003d0.0232\n",
            "6: IZY\u003d3.19\tIZX\u003d77.62\tacc\u003d0.9720\tavg_acc\u003d0.9773\terr\u003d0.0280\tavg_err\u003d0.0227\n",
            "7: IZY\u003d3.19\tIZX\u003d66.89\tacc\u003d0.9742\tavg_acc\u003d0.9803\terr\u003d0.0258\tavg_err\u003d0.0197\n",
            "8: IZY\u003d3.20\tIZX\u003d69.95\tacc\u003d0.9740\tavg_acc\u003d0.9810\terr\u003d0.0260\tavg_err\u003d0.0190\n",
            "9: IZY\u003d3.20\tIZX\u003d62.87\tacc\u003d0.9758\tavg_acc\u003d0.9810\terr\u003d0.0242\tavg_err\u003d0.0190\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(10):\n",
        "    for step in range(steps_per_batch):\n",
        "        im, ls \u003d mnist_data.train.next_batch(batch_size)\n",
        "        sess.run(train_tensor, feed_dict\u003d{images: im, labels: ls})\n",
        "    print(\"{}: IZY\u003d{:.2f}\\tIZX\u003d{:.2f}\\tacc\u003d{:.4f}\\tavg_acc\u003d{:.4f}\\terr\u003d{:.4f}\\tavg_err\u003d{:.4f}\".format(epoch, *evaluate()))\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "savepth \u003d saver.save(sess, \u0027/tmp/mnistvib\u0027, global_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\Student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/mnistvib-6000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(3.228585,\n",
              " 71.078026,\n",
              " 0.9798,\n",
              " 0.9839,\n",
              " 0.020200014114379883,\n",
              " 0.016099989414215088)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "saver_polyak.restore(sess, savepth)\n",
        "evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /tmp/mnistvib-6000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(3.1953857,\n",
              " 62.871254,\n",
              " 0.9749,\n",
              " 0.9821,\n",
              " 0.025099992752075195,\n",
              " 0.01789999008178711)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "saver.restore(sess, savepth)\n",
        "evaluate()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}